{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><p style=\"font-size: XX-large\"><font color = \"green\">Retrieval-Augmented Generation for United States Congressional Bills and Laws (Pre-Processing)</font></p></b> </div>\n",
    "<b><p style=\"font-size: X-large\"><font color = \"green\">Andrew Cai (cai.and@northeastern.edu)</font></p></b> </div>\n",
    "<b><p style=\"font-size: X-large\"><font color = \"green\">Joynae Whitehurst (whitehurst.j@northeastern.edu)</font></p></b> </div>\n",
    "<b><p style=\"font-size: X-large\"><font color = \"green\">Lili Xiang (xiang.l@northeastern.edu)</font></p></b> </div>\n",
    "<b><p style=\"font-size: X-large\"><font color = \"green\">April 24, 2025</font></p></b> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Retrieval\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "\n",
    "# Nice to have:\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Text Cleaning\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import html\n",
    "import unicodedata\n",
    "from ftfy import fix_text\n",
    "\n",
    "# Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast  # To safely evaluate string representations of lists\n",
    "\n",
    "# Chunking\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Topic Tagging/Embedding\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "import torch\n",
    "import faiss\n",
    "import accelerate\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Pre-Trained Model Retrieval\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bill File Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download and extract bill summaries\n",
    "def download_and_extract_bills(congress, bill_type, save_dir=\"congressional_bills\"):\n",
    "    \"\"\"Retrieve and download bills from govinfo.gov as XML files\n",
    "\n",
    "    Args:\n",
    "        congress (int): Congress number.\n",
    "        bill_type (str): Bill category.\n",
    "        save_dir (str, optional): Directory where files will be stored. Defaults to \"congressional_bills\".\n",
    "    \"\"\"\n",
    "    # Base URL for downloading bill summaries\n",
    "    BASE_URL = \"https://www.govinfo.gov/bulkdata/BILLSUM/{congress}/{bill_type}/BILLSUM-{congress}-{bill_type}.zip\"\n",
    "\n",
    "    # Construct URL\n",
    "    url = BASE_URL.format(congress=congress, bill_type=bill_type)\n",
    "    \n",
    "    # Create the base directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Define path for ZIP file\n",
    "    zip_filename = f\"BILLSUM-{congress}-{bill_type}.zip\"\n",
    "    zip_path = os.path.join(save_dir, zip_filename)\n",
    "    \n",
    "    print(f\"Downloading: {url}\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Save ZIP file\n",
    "        with open(zip_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Saved ZIP: {zip_path}\")\n",
    "\n",
    "        # Create extraction folder named after the ZIP file (without .zip)\n",
    "        extract_folder = os.path.join(save_dir, zip_filename.replace(\".zip\", \"\"))\n",
    "        os.makedirs(extract_folder, exist_ok=True)  # Ensure folder exists\n",
    "\n",
    "        # Extract ZIP contents into the specific folder\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(extract_folder)\n",
    "        \n",
    "        print(f\"Extracted to: {extract_folder}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"Failed to download {url}. Status Code: {response.status_code}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove problematic keyword arguments if present, which found during debugging\n",
    "_original_accelerator_init = accelerate.Accelerator.__init__\n",
    "\n",
    "def new_accelerator_init(self, *args, **kwargs):\n",
    "    \"\"\"Accelerator initialization\n",
    "    \"\"\"\n",
    "    for key in ['dispatch_batches', 'even_batches', 'use_seedable_sampler']:\n",
    "        kwargs.pop(key, None)\n",
    "    _original_accelerator_init(self, *args, **kwargs)\n",
    "\n",
    "accelerate.Accelerator.__init__ = new_accelerator_init\n",
    "\n",
    "# set up the device to accommodate difference: use the MPS backend if available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# choose smallest Flan-T5\n",
    "model_name = \"google/flan-t5-small\"\n",
    "\n",
    "# load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "\n",
    "# load the dataset from Hugging Face\n",
    "dataset = load_dataset(\"dreamproit/bill_labels_us\")\n",
    "\n",
    "# define maximum sequence lengths for inputs and targets\n",
    "max_input_length = 512\n",
    "max_target_length = 64\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"preprocessing function to tokenize inputs (bill text) and targets (policy area)\n",
    "\n",
    "    Args:\n",
    "        examples (str): String of text\n",
    "\n",
    "    Returns:\n",
    "        model inputs: Model labels for input\n",
    "    \"\"\"\n",
    "    inputs = examples[\"text\"]\n",
    "    targets = examples[\"policy_area\"]\n",
    "    # Tokenize the input text (bill content)\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    # Tokenize the target labels (policy tags)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# tokenize the entire dataset using multiple processes for speedup\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, num_proc=4)\n",
    "\n",
    "# in case the dataset doesn't have an explicit 'train' and 'test' split\n",
    "if \"train\" not in tokenized_dataset.keys() or \"test\" not in tokenized_dataset.keys():\n",
    "    tokenized_dataset = tokenized_dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "\n",
    "# for MPS, disable mixed precision flags (bf16/fp16)\n",
    "if device.type == \"mps\":\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=\"./flan-t5-finetuned-bill_labels\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        save_strategy=\"epoch\",\n",
    "        predict_with_generate=True,\n",
    "        fp16=False,\n",
    "    )\n",
    "else:\n",
    "    # bf16 for supported CPU devices \n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=\"./flan-t5-finetuned-bill_labels\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        save_strategy=\"epoch\",\n",
    "        predict_with_generate=True,\n",
    "        bf16=True,  \n",
    "    )\n",
    "\n",
    "# create a data collator that dynamically pads the inputs\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# initialize the Seq2SeqTrainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# start training\n",
    "trainer.train()\n",
    "\n",
    "# save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(\"./flan-t5-finetuned-bill_labels\")\n",
    "tokenizer.save_pretrained(\"./flan-t5-finetuned-bill_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned tokenizer and model\n",
    "def load_model(path=\"./checkpoint-26904\"):\n",
    "    \"\"\"Load in pre-trained topic modeler\n",
    "\n",
    "    Args:\n",
    "        path (str, optional): Path to saved model. Defaults to \"./checkpoint-26904\".\n",
    "\n",
    "    Returns:\n",
    "        tokenizer and model for topic tagging\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(path)\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "# Perform inference for multiple topic tags\n",
    "def infer_topics(tokenizer, model, text, top_k=5):\n",
    "    \"\"\"Generate list of topics matching body of text\n",
    "\n",
    "    Args:\n",
    "        tokenizer (tokenizer): Topic tokenizer\n",
    "        model (model): Topic tag model\n",
    "        text (str): Text to be tagged\n",
    "        top_k (int, optional): Top topic matches. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        topics: A list of strings of topics\n",
    "    \"\"\"\n",
    "    in_tensor = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            in_tensor.input_ids, \n",
    "            max_length=64, \n",
    "            num_beams=10, # Max topics\n",
    "            num_return_sequences=top_k,  # Generate multiple outputs\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    topics = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "    return topics  # Return a list of topics\n",
    "\n",
    "# Load model once\n",
    "tokenizer, model = load_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Removes HTML tags, decodes HTML entities, normalizes whitespace, and expands abbreviations.\n",
    "\n",
    "    Args:\n",
    "        text (str): Body of text\n",
    "\n",
    "    Returns:\n",
    "        text: Cleaned body of text\n",
    "    \"\"\"\n",
    "    text = fix_text(text)  # Fix encoding issues\n",
    "    text = unicodedata.normalize(\"NFKC\", text)  # Normalize Unicode characters\n",
    "    text = re.sub(r'<.*?>', ' ', text)  # Remove HTML tags\n",
    "    text = html.unescape(text)  # Decode HTML entities (&nbsp;, &amp;, etc.)\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize spaces\n",
    "    text = re.sub(r'[“”‘’]', '\"', text)  # Normalize quotation marks\n",
    "    text = re.sub(r\"[^\\w\\s.,;:'\\\"!?-]\", '', text)  # Remove unexpected symbols (except punctuation)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Chunking Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=500, overlap=50):\n",
    "    \"\"\"Chunk text for easier LLM processing\n",
    "\n",
    "    Args:\n",
    "        text (str): A body of text\n",
    "        chunk_size (int, optional): How much text in a chunk. Defaults to 500.\n",
    "        overlap (int, optional): Overlap between chunks. Defaults to 50.\n",
    "\n",
    "    Returns:\n",
    "        Chunked texts from big body of text based on size and overlap\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "    return text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File and Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_bill_parse(xml_file):\n",
    "    \"\"\"Parse through bills XML to retrieve, tag, chunk relevant data\n",
    "\n",
    "    Args:\n",
    "        xml_file (XML): Bill in XML form\n",
    "\n",
    "    Returns:\n",
    "        bill_data: List of bill info\n",
    "    \"\"\"\n",
    "    # XML variables\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Convert bill category from nickname to actual name\n",
    "    bill_category = [\"hconres\", \"hjres\", \"hr\", \"hres\", \"s\", \"sconres\", \"sjres\", \"sres\"]\n",
    "    full_name = [\"House Concurrent Resolution\", \"House Joint Resolution\", \"House Bill\", \"House Simple Resolution\", \"Senate Bill\", \"Senate Concurrent Resolution\", \"Senate Joint Resolution\", \"Senate Simple Resolution\"]\n",
    "    \n",
    "    # Initialize empty list to store data\n",
    "    bill_data = []\n",
    "    \n",
    "    # Data parsing\n",
    "    for item in root.findall(\"item\"):\n",
    "        summary_text = clean_text(item.find(\".//summary-text\").text.strip()) if item.find(\".//summary-text\") is not None else None\n",
    "        title = clean_text(item.find(\"title\").text.strip()) if item.find(\"title\") is not None else None\n",
    "        topic_tag = infer_topics(tokenizer, model, summary_text, 10) if summary_text else [\"Unknown\"]\n",
    "        text_chunks = chunk_text(summary_text) if summary_text else []\n",
    "        measure_type = full_name[bill_category.index(item.get(\"measure-type\"))]\n",
    "        # Each chunk gets its own row\n",
    "        for chunk in text_chunks:\n",
    "            bill_info = {\n",
    "                \"congress\": item.get(\"congress\"),\n",
    "                \"measure_type\": measure_type,\n",
    "                \"measure_number\": item.get(\"measure-number\"),\n",
    "                \"measure_id\": item.get(\"measure-id\"),\n",
    "                \"origin_chamber\": item.get(\"originChamber\"),\n",
    "                \"current_chamber\": item.find(\"summary\").get(\"currentChamber\"),\n",
    "                \"orig_publish_date\": item.get(\"orig-publish-date\"),\n",
    "                \"update_date\": item.get(\"update-date\"),\n",
    "                \"title\": title,\n",
    "                \"action_date\": item.find(\".//action-date\").text.strip() if item.find(\".//action-date\") is not None else None,\n",
    "                \"action_desc\": item.find(\".//action-desc\").text.strip() if item.find(\".//action-desc\") is not None else None,\n",
    "                \"summary_text\": summary_text,\n",
    "                \"topic_tags\": topic_tag,  # Multiple topics as a list\n",
    "                \"text_chunk\": chunk  # One chunk per row\n",
    "            }\n",
    "            bill_data.append(bill_info)\n",
    "\n",
    "    return bill_data  # Return list of expanded rows\n",
    "\n",
    "\n",
    "def process_bills(directory):\n",
    "    \"\"\"Parse through all XML files and compile data into a CSV\n",
    "\n",
    "    Args:\n",
    "        directory (file path): Where main folder that houses the other folders\n",
    "    \"\"\"\n",
    "    print(\"Files and directories in:\", directory, os.listdir(directory))  # Debugging\n",
    "\n",
    "    # Initialzie empty lists\n",
    "    all_bills = []\n",
    "    xml_files = []\n",
    "    \n",
    "    # Find XML files\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".xml\"):\n",
    "                xml_files.append(os.path.join(root, filename))\n",
    "\n",
    "    # Use tqdm and suppress inner function prints\n",
    "    for file_path in tqdm(xml_files, desc=\"Processing Bills\", unit=\"file\", leave=True):\n",
    "        try:\n",
    "            bills = xml_bill_parse(file_path)  # Extract data from XML\n",
    "            all_bills.extend(bills)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "    # Convert data in dataframe\n",
    "    df = pd.DataFrame(all_bills)\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"No data extracted! Check file paths and XML structure.\")\n",
    "    else:\n",
    "        print(f\"Successfully processed {len(df)} rows.\")\n",
    "\n",
    "    df.to_csv(\"parsed_bills.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: https://www.govinfo.gov/bulkdata/BILLSUM/115/hconres/BILLSUM-115-hconres.zip\n",
      "Saved ZIP: congressional_bills\\BILLSUM-115-hconres.zip\n",
      "Extracted to: congressional_bills\\BILLSUM-115-hconres\n",
      "Downloading: https://www.govinfo.gov/bulkdata/BILLSUM/115/hjres/BILLSUM-115-hjres.zip\n",
      "Saved ZIP: congressional_bills\\BILLSUM-115-hjres.zip\n",
      "Extracted to: congressional_bills\\BILLSUM-115-hjres\n",
      "Downloading: https://www.govinfo.gov/bulkdata/BILLSUM/115/hr/BILLSUM-115-hr.zip\n",
      "Saved ZIP: congressional_bills\\BILLSUM-115-hr.zip\n",
      "Extracted to: congressional_bills\\BILLSUM-115-hr\n",
      "Downloading: https://www.govinfo.gov/bulkdata/BILLSUM/115/hres/BILLSUM-115-hres.zip\n",
      "Saved ZIP: congressional_bills\\BILLSUM-115-hres.zip\n",
      "Extracted to: congressional_bills\\BILLSUM-115-hres\n",
      "Downloading: https://www.govinfo.gov/bulkdata/BILLSUM/115/s/BILLSUM-115-s.zip\n",
      "Saved ZIP: congressional_bills\\BILLSUM-115-s.zip\n",
      "Extracted to: congressional_bills\\BILLSUM-115-s\n",
      "Downloading: https://www.govinfo.gov/bulkdata/BILLSUM/115/sconres/BILLSUM-115-sconres.zip\n",
      "Saved ZIP: congressional_bills\\BILLSUM-115-sconres.zip\n",
      "Extracted to: congressional_bills\\BILLSUM-115-sconres\n",
      "Downloading: https://www.govinfo.gov/bulkdata/BILLSUM/115/sjres/BILLSUM-115-sjres.zip\n",
      "Saved ZIP: congressional_bills\\BILLSUM-115-sjres.zip\n",
      "Extracted to: congressional_bills\\BILLSUM-115-sjres\n",
      "Downloading: https://www.govinfo.gov/bulkdata/BILLSUM/115/sres/BILLSUM-115-sres.zip\n",
      "Saved ZIP: congressional_bills\\BILLSUM-115-sres.zip\n",
      "Extracted to: congressional_bills\\BILLSUM-115-sres\n"
     ]
    }
   ],
   "source": [
    "# Directory where bills are extracted\n",
    "BILLS_DIR = \"congressional_bills\"\n",
    "\n",
    "# Congress' of interest\n",
    "congress_number = [115, 116, 117, 118, 119]\n",
    "\n",
    "# Bill Categories\n",
    "bill_category = [\"hconres\", \"hjres\", \"hr\", \"hres\", \"s\", \"sconres\", \"sjres\", \"sres\"]\n",
    "\n",
    "# Download Bills\n",
    "for num in congress_number:\n",
    "    for cat in bill_category:\n",
    "        download_and_extract_bills(num, cat, save_dir=BILLS_DIR)\n",
    "\n",
    "# Run the function to process bills\n",
    "process_bills(BILLS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121238\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of         congress                 measure_type  measure_number  \\\n",
       "0            115  House Concurrent Resolution               1   \n",
       "1            115  House Concurrent Resolution              10   \n",
       "2            115  House Concurrent Resolution             100   \n",
       "3            115  House Concurrent Resolution             100   \n",
       "4            115  House Concurrent Resolution             101   \n",
       "...          ...                          ...             ...   \n",
       "121233       119     Senate Simple Resolution               8   \n",
       "121234       119     Senate Simple Resolution              81   \n",
       "121235       119     Senate Simple Resolution              81   \n",
       "121236       119     Senate Simple Resolution              88   \n",
       "121237       119     Senate Simple Resolution               9   \n",
       "\n",
       "             measure_id origin_chamber current_chamber orig_publish_date  \\\n",
       "0         id115hconres1          HOUSE           HOUSE        2017-01-03   \n",
       "1        id115hconres10          HOUSE           HOUSE        2017-01-13   \n",
       "2       id115hconres100          HOUSE           HOUSE        2018-01-22   \n",
       "3       id115hconres100          HOUSE           HOUSE        2018-01-22   \n",
       "4       id115hconres101          HOUSE          SENATE        2018-01-25   \n",
       "...                 ...            ...             ...               ...   \n",
       "121233       id119sres8         SENATE          SENATE        2025-01-03   \n",
       "121234      id119sres81         SENATE          SENATE        2025-02-13   \n",
       "121235      id119sres81         SENATE          SENATE        2025-02-13   \n",
       "121236      id119sres88         SENATE          SENATE        2025-02-21   \n",
       "121237       id119sres9         SENATE          SENATE        2025-01-03   \n",
       "\n",
       "       update_date                                              title  \\\n",
       "0       2017-01-23  Regarding consent to assemble outside the seat...   \n",
       "1       2017-01-17  Expressing the sense of the Congress that tax-...   \n",
       "2       2018-05-08  Providing for a correction in the enrollment o...   \n",
       "3       2018-05-08  Providing for a correction in the enrollment o...   \n",
       "4       2018-02-02  Providing for a joint session of Congress to r...   \n",
       "...            ...                                                ...   \n",
       "121233  2025-01-30  A resolution electing Jackie Barber as Secreta...   \n",
       "121234  2025-03-18  A resolution calling on the United Kingdom, Fr...   \n",
       "121235  2025-03-18  A resolution calling on the United Kingdom, Fr...   \n",
       "121236  2025-03-31  A resolution designating March 7, 2025, as \"Na...   \n",
       "121237  2025-01-30  A resolution notifying the President of the Un...   \n",
       "\n",
       "       action_date                      action_desc  \\\n",
       "0       2017-01-03              Introduced in House   \n",
       "1       2017-01-13              Introduced in House   \n",
       "2       2018-01-22              Introduced in House   \n",
       "3       2018-01-22              Introduced in House   \n",
       "4       2018-01-29  Passed Senate without amendment   \n",
       "...            ...                              ...   \n",
       "121233  2025-01-03             Introduced in Senate   \n",
       "121234  2025-02-13             Introduced in Senate   \n",
       "121235  2025-02-13             Introduced in Senate   \n",
       "121236  2025-02-21             Introduced in Senate   \n",
       "121237  2025-01-03             Introduced in Senate   \n",
       "\n",
       "                                             summary_text  \\\n",
       "0       Authorizes the Speaker of the House and the Ma...   \n",
       "1       Expresses the sense of Congress that the tax e...   \n",
       "2       Directs the Clerk of the House of Representati...   \n",
       "3       Directs the Clerk of the House of Representati...   \n",
       "4       This measure has not been amended since it was...   \n",
       "...                                                   ...   \n",
       "121233  This resolution elects Jackie Barber as Secret...   \n",
       "121234  The resolution urges the E3 the United Kingdom...   \n",
       "121235  The resolution urges the E3 the United Kingdom...   \n",
       "121236  This resolution designates March 7, 2025, as N...   \n",
       "121237  This resolution resolves that the President be...   \n",
       "\n",
       "                                               topic_tags  \\\n",
       "0       ['Congress', 'Government Operations and Politi...   \n",
       "1       ['Taxation', 'Social Welfare', 'Commemorations...   \n",
       "2       ['Congress', 'Economics and Public Finance', '...   \n",
       "3       ['Congress', 'Economics and Public Finance', '...   \n",
       "4       ['Congress', 'Commemorations', 'Agriculture an...   \n",
       "...                                                   ...   \n",
       "121233  ['Congress', 'Commemorations', 'Social Science...   \n",
       "121234  ['International Affairs', 'Security and Intern...   \n",
       "121235  ['International Affairs', 'Security and Intern...   \n",
       "121236  ['Education', 'Government Operations and Polit...   \n",
       "121237  ['Congress', 'Commemorations', 'Social Science...   \n",
       "\n",
       "                                               text_chunk  \n",
       "0       Authorizes the Speaker of the House and the Ma...  \n",
       "1       Expresses the sense of Congress that the tax e...  \n",
       "2       Directs the Clerk of the House of Representati...  \n",
       "3       a lapse in FY2018 appropriations; and 4 specif...  \n",
       "4       This measure has not been amended since it was...  \n",
       "...                                                   ...  \n",
       "121233  This resolution elects Jackie Barber as Secret...  \n",
       "121234  The resolution urges the E3 the United Kingdom...  \n",
       "121235  3 reaffirms that the United States maintains t...  \n",
       "121236  This resolution designates March 7, 2025, as N...  \n",
       "121237  This resolution resolves that the President be...  \n",
       "\n",
       "[121238 rows x 14 columns]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# General file name\n",
    "save_df = f\"parsed_bills_{congress_number[0]}-{congress_number[-1]}\"\n",
    "\n",
    "# Read in and rename parsed file\n",
    "df_load = pd.read_csv(\"parsed_bills.csv\")\n",
    "df_load.to_csv(save_df, index=False)\n",
    "\n",
    "# Lower file size by removing summary text\n",
    "df_load_chunk = df_load.drop('summary_text', axis=1)\n",
    "df_load_chunk.to_csv(save_df + \"_chunks_only.csv\" , index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding and Indexing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriee only chunk dataframe\n",
    "df_chunk = df_load_chunk\n",
    "\n",
    "# Load Sentence Transformer for embeddings\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Convert text chunks to embeddings using batch processing\n",
    "embeddings = embed_model.encode(df_chunk[\"text_chunk\"].tolist(), batch_size=32, show_progress_bar=True)\n",
    "\n",
    "# Convert the 2D NumPy array into a list of lists for Pandas\n",
    "df_chunk[\"embedding\"] = embeddings.tolist()\n",
    "\n",
    "# Save to CSV\n",
    "df_chunk.to_csv(save_df + \"_chunks_only_embedded.csv\", index=False)\n",
    "\n",
    "# Convert stored string embeddings back to NumPy arrays\n",
    "df_chunk[\"embedding\"] = df_chunk[\"embedding\"].apply(lambda x: np.array(ast.literal_eval(x), dtype=np.float32))\n",
    "\n",
    "# Get embedding dimension\n",
    "embedding_dim = df_chunk[\"embedding\"][0].shape[0]\n",
    "\n",
    "# Initialize FAISS index\n",
    "faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# Convert embeddings to a NumPy array\n",
    "embeddings = np.vstack(df_chunk[\"embedding\"].values)\n",
    "\n",
    "# Batch size for adding embeddings\n",
    "batch_size = 1000\n",
    "\n",
    "# Add embeddings in batches with progress tracking\n",
    "for i in tqdm(range(0, len(embeddings), batch_size), desc=\"Indexing embeddings\"):\n",
    "    faiss_index.add(embeddings[i : i + batch_size])\n",
    "\n",
    "# Save FAISS index\n",
    "faiss.write_index(faiss_index, \"bill_embeddings.index\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
