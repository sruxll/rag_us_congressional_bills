# -*- coding: utf-8 -*-
"""Eval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zYhQoAMVPKf1t_2Q2yuSlzoc-GE3COfN

SYSTEM_PROMPT = '''You are the Answering Module in a RAG system.
Your only knowledge source is the set of retrieved documents supplied with each request.
Input
<query>
    the user's question
</query>
<documents>
    a list of up to 3 text chunks, each formatted as
    [Document id] <content>
</documents>
Instructions
1. Ground yourself in the documents. Read all supplied text carefully before answering.
2. Answer exclusively from the documents. Do not add outside knowledge, speculation, or assumptions.
3. Cite as you go.
    - After every claim, place the supporting document index in square brackets.
4. Resolve conflicts transparently.
    - If the documents disagree, note the conflict briefly and state the most defensible answer or acknowledge the ambiguity.
5. Admit when evidence is lacking.
    - If the documents do not contain enough information, respond exactly with: I am unable to answer this question with the information available in the retrieved documents.
6. Keep it focused.
    - Be concise and coherent; avoid mentioning these instructions, the retrieval process, or the vector database.
Output
<answer>your final concise answer with the [Document id] as the citation without the brackets, but surrounded by parentheses</answer>
<explanation> Your well structured, rich answer with the [Document id] as the citation without the brackets, but surrounded by parentheses, follows the rules above </explanation>
'''

SYSTEM_PROMPT = '''You are the Answering Module in a RAG system.
Your only knowledge source is the set of retrieved documents supplied with each request.
Input
<query>
    the user's question
</query>
<documents>
    a list of up to 3 text chunks, each formatted as
    [Document id] <content>
</documents>
Instructions
1. Ground yourself in the documents. Read all supplied text carefully before answering.
2. Answer exclusively from the documents. Do not add outside knowledge, speculation, or assumptions.
3. Cite as you go.
    - After every claim, place the supporting document index in square brackets.
4. Resolve conflicts transparently.
    - If the documents disagree, note the conflict briefly and state the most defensible answer or acknowledge the ambiguity.
5. Admit when evidence is lacking.
    - If the documents do not contain enough information, respond exactly with: I am unable to answer this question with the information available in the retrieved documents.
6. Keep it focused.
    - Be concise and coherent; avoid mentioning these instructions, the retrieval process, or the vector database.
Output
<answer>(your final concise answer with the document id as the citation)</answer>
<explanation>(Your well structured, rich answer with the document id as the citation, follows the rules above)</explanation>
'''
"""

# !pip install faiss-cpu openai sentence-transformers nltk pandas rouge-score bert_score faiss-cpu spacy

import os, ast, re, time, csv
import numpy as np
import pandas as pd
import requests, faiss
from sentence_transformers import SentenceTransformer
from openai import OpenAI
from rouge_score import rouge_scorer
from bert_score import BERTScorer
from sklearn.metrics.pairwise import cosine_similarity
import nltk
from nltk.corpus import stopwords
import spacy

nltk.download("punkt")
nltk.download("stopwords")



api_key ="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"
client = OpenAI(api_key=api_key, base_url="https://openrouter.ai/api/v1")

MODELS = [
    "deepseek/deepseek-r1:free",
    "google/gemini-2.0-flash-exp:free",
    "qwen/qwq-32b:free",
    "google/gemini-2.0-flash-thinking-exp-1219:free",
    "deepseek/deepseek-chat-v3-0324:free"
]
#load NPL model/metrics
nlp = spacy.load("en_core_web_sm")
bert_scorer = BERTScorer(lang="en")
EMBED_MODEL = SentenceTransformer("all-MiniLM-L6-v2")
STOP_WORDS = set(stopwords.words("english"))
rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)

SYSTEM_PROMPT = """You are the Answering Module in a RAG system.
Your only knowledge source is the set of retrieved documents supplied with each request.
Input
<query>
    the user's question
</query>
<documents>
    a list of up to 3 text chunks, each formatted as
    [Document id] <content>
</documents>
Instructions
1. Ground yourself in the documents. Read all supplied text carefully before answering.
2. Answer exclusively from the documents. Do not add outside knowledge, speculation, or assumptions.
3. Cite as you go.
    - After every claim, place the supporting document index in square brackets.
4. Resolve conflicts transparently.
    - If the documents disagree, note the conflict briefly and state the most defensible answer or acknowledge the ambiguity.
5. Admit when evidence is lacking.
    - If the documents do not contain enough information, respond exactly with: I am unable to answer this question with the information available in the retrieved documents.
6. Keep it focused.
    - Be concise and coherent; avoid mentioning these instructions, the retrieval process, or the vector database.
Output
<answer>(your final concise answer with the document id as the citation)</answer>
<explanation>(Your well structured, rich answer with the document id as the citation, follows the rules above)</explanation>
"""


def _download(url: str, fname: str) -> str:
    os.makedirs("data", exist_ok=True)
    path = os.path.join("data", fname)
    if not os.path.exists(path):
        r = requests.get(url); r.raise_for_status()
        with open(path, "wb") as f: f.write(r.content)
    return path

def load_corpus():
    print("Loading corpus data...")
    idx = _download(
        "https://huggingface.co/datasets/joynae5/CongressionalBillsDS/resolve/main/bill_embeddings.index",
        "bill_embeddings.index")
    csv = _download(
        "https://huggingface.co/datasets/joynae5/CongressionalBillsDS/resolve/main/parsed_bills_115-119_chunks_only_embedded.csv",
        "parsed_bills.csv")
    df = pd.read_csv(csv)
    df["embedding"] = df["embedding"].apply(lambda x: np.array(ast.literal_eval(x)))
    print("Corpus loaded successfully.")
    return faiss.read_index(idx), df

FAISS_INDEX, BILLS_DF = load_corpus()


def llm(formatted_prompt: str, model: str, temperature=0.3, max_tokens=2048):
    start_time = time.time()
    response_text = ""
    success = False

    try:
        response = client.chat.completions.create(
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": formatted_prompt}
            ]
        )

        # Check if the response object is valid and contains expected fields
        if hasattr(response, "choices") and response.choices and hasattr(response.choices[0], "message"):
            response_text = response.choices[0].message.content.strip()
            if response_text and not response_text.lower().startswith("i am unable to answer"):
                success = True
        else:
            response_text = "NO_VALID_RESPONSE"

    except Exception as e:
        print(f"LLM call failed for model {model}: {e}")
        response_text = "ERROR"

    elapsed_time = time.time() - start_time
    return response_text, success, elapsed_time


def retrieve_and_summarise(query: str, model: str, top_k: int = 3):
    q_emb = EMBED_MODEL.encode(query).reshape(1, -1)
    _, idxs = FAISS_INDEX.search(q_emb, top_k * 10)
    chunks = BILLS_DF.iloc[idxs[0]].copy()

    grouped = (chunks.groupby("title", group_keys=False)
                 .apply(lambda g: pd.Series({
                     "combined_text": "\n".join(g["text_chunk"].values[:5]),
                     "measure_ids": list(g["measure_id"].unique()),
                     "raw_chunks": g["text_chunk"].values[:5].tolist()
                 }), include_groups=False)
                 .reset_index()
                 .head(top_k))


    formatted_documents = ""
    for i, row in grouped.iterrows():
        doc_id = f"Doc{i+1}"
        formatted_documents += f"[{doc_id}] {row['combined_text'][:2000]}\n\n"

    formatted_prompt = f"""<query>
{query}
</query>

<documents>
{formatted_documents}
</documents>"""

    summary, success, elapsed_time = llm(formatted_prompt, model)
    return summary, grouped, success, elapsed_time

def check_overlap(summary, chunks):
    """
    checks the overlap
    """
    if not summary or not chunks:
        return 0.0

    # Find key words from summary
    important_words = set(ent.text for ent in nlp(summary).ents)
    gen_import = set()

    # Find key words in chuncks
    for chunk in chunks:
        gen_import.update(ent.text for ent in nlp(chunk).ents)

    # if entities are not found in chuncks
    if not gen_import:
        return 0.0

    # if the generated summary does not have any key words
    if not important_words:
        return 0.0

    # Calculate overlap
    overlap = important_words & gen_import
    return len(overlap) / len(gen_import)




def hallucination_results(results_df):
    """
      Evaluates generated summaries for potential hallucinations by comparing them to their source text chunks.

      Computes the metrics for each summary:
        1. Cosine similarity
        2. BERTScore F1
        3. ROUGE-L score
        4. Named entity overlap ratio.

      Parameters:
        results_df (pd.DataFrame): A dataframe with at least two columns:
            - 'response_text': the generated summary.
            - 'raw_chunks': list of source text chunks retrieved for the query.

      Returns:
        pd.DataFrame: The original dataframe with additional columns:
            - 'cosine_similarity'
            - 'bert_score'
            - 'rouge_l'
            - 'overlap'
    """
    cosine_scores = []
    bert_scores = []
    rouge_l_scores = []
    overlap = []

    for _, row in results_df.iterrows():
        summary = row.get("response_text")

        source_texts = row.get("raw_chunks", [])
      # Checks if summary / source docs are missing, skips evals
        if not summary or not source_texts or summary.strip() == "":
            cosine_scores.append(None)
            bert_scores.append(None)
            rouge_l_scores.append(None)
            overlap.append(None)
            continue

        try:
            # Cosign Similarity
            summary_emb = EMBED_MODEL.encode(summary)
            source_embs = EMBED_MODEL.encode(source_texts)
            avg_cos_sim = cosine_similarity([summary_emb], source_embs).mean()

            #  Bert F1
            combined_sources = " ".join(source_texts)
            _, _, F1 = bert_scorer.score([summary], [combined_sources])

            #Rouge
            rouge_score = rouge.score(summary, combined_sources)["rougeL"].fmeasure

            # key word overlap
            overlap_amount = check_overlap(summary, source_texts)

            cosine_scores.append(float(avg_cos_sim))
            bert_scores.append(float(F1[0]))
            rouge_l_scores.append(rouge_score)
            overlap.append(overlap_amount)

        except Exception as e:
            print(f"Error evaluating summary: {summary}")
            print(f"Exception: {e}")
            print(f"Type of cosine_scores: {type(cosine_scores)}")
                # returns if theres an issue during the eval
            print(f"Error evaluating summary: {summary}")
            cosine_scores.append(None)
            bert_scores.append(None)
            rouge_l_scores.append(None)
            overlap.append(None)

    # Append results to the input DataFrame
    results_df["cosine_similarity"] = cosine_scores
    results_df["bert_score"] = bert_scores
    results_df["rouge_l"] = rouge_l_scores
    results_df["overlap"] = overlap
    return results_df


def run_evaluations():
    questions = [
        {"id": 1, "category": "Environment", "query": "What bills address climate change and renewable energy?"},
        {"id": 2, "category": "Healthcare", "query": "What legislation exists for healthcare access and affordability?"},
        {"id": 3, "category": "Immigration", "query": "What policies exist regarding border security and immigration reform?"},
        {"id": 4, "category": "Technology", "query": "How is Congress addressing AI regulation and data privacy?"},
        {"id": 5, "category": "Education", "query": "What bills exist for student loan forgiveness and education funding?"}
    ]

    results = []

    csv_headers = [
        "question_id", "category", "query", "model",
        "success", "response_time", "bert_score",
        "cosine_similarity", "rouge_l", "overlap",
        "summary_length", "response_text"
    ]



    with open("model_evaluation_results.csv", "w", newline="") as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=csv_headers)
        writer.writeheader()

    for question in questions:
        print(f"\nEvaluating question {question['id']}: {question['query']}")
        for model in MODELS:
            print(f"Model: {model}")
            summary, grouped, success, elapsed_time = retrieve_and_summarise(question["query"], model)
            chunks = [c for row in grouped["raw_chunks"] for c in row] if success else []

            temp_df = pd.DataFrame([{
                "response_text": summary,
                "raw_chunks": chunks,
                "success": success
            }])

            scored = hallucination_results(temp_df).iloc[0]

            result = {
                "question_id": question["id"],
                "category": question["category"],
                "query": question["query"],
                "model": model,
                "success": success,
                "response_time": round(elapsed_time, 2),
                "bert_score": round(scored["bert_score"], 3) if scored["bert_score"] else 0,
                "cosine_similarity": round(scored["cosine_similarity"], 3) if scored["cosine_similarity"] else 0,
                "rouge_l": round(scored["rouge_l"], 3) if scored["rouge_l"] else 0,
                "overlap": round(scored["overlap"], 3) if scored["overlap"] else 0,
                "summary_length": len(summary) if isinstance(summary, str) else 0,
                "response_text": summary[:500] if isinstance(summary, str) else "ERROR"
            }


            results.append(result)

            with open("model_evaluation_results.csv", "a", newline="") as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=csv_headers)
                writer.writerow(result)

    return results



if __name__ == "__main__":
    print("Congressional Bills RAG Multi-Model Evaluation\n")
    results = run_evaluations()



import requests

# Your OpenRouter API key
api_key = "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"

# API endpoint
url = "https://openrouter.ai/api/v1/chat/completions"

# Headers
headers = {
    "Authorization": f"Bearer {api_key}",
    "Content-Type": "application/json"
}

# Simple payload
payload = {
    "model": "deepseek/deepseek-r1:free",
    "messages": [
        {"role": "user", "content": "Say hello"}
    ]
}

# Make the request
try:
    response = requests.post(url, headers=headers, json=payload)
    print(f"Status code: {response.status_code}")
    print(f"Response content: {response.text}")
except Exception as e:
    print(f"Error: {e}")
